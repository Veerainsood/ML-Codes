from tqdm import tqdm

from torch.utils.data import Dataset,DataLoader,Subset
from torch.optim import Adam
import torch as torch
from torchvision.datasets import MNIST
from torchvision import transforms



transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((.5,),(.5,)),
])


train_set = MNIST('./data',train=True,download=True,transform=transform)

test_set = MNIST('./data',train=False,transform=transform,download=True)


train_loader = DataLoader(train_set,batch_size=64,shuffle=True)
test_loader = DataLoader(test_set,batch_size=64,shuffle=False)


class Generator(torch.nn.Module):
    def __init__(self,latent_dim, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.predictor = torch.nn.Sequential(
            torch.nn.ConvTranspose2d(latent_dim,16,kernel_size=4,stride=2), # (n-1)*stride + f + outputPad -> (1-1)*2 + 4 + 0
            torch.nn.BatchNorm2d(16),
            torch.nn.ReLU(),

            torch.nn.ConvTranspose2d(16,8,kernel_size=7,stride=2), # (4-1)*2 + 7
            torch.nn.BatchNorm2d(8),
            torch.nn.ReLU(),

            torch.nn.ConvTranspose2d(8,1,kernel_size=4,stride=2), # (13-1)*2 + 4 = 28
            torch.nn.Tanh(),
        )

    def forward(self,x):
        return self.predictor(x)

class Descriminator(torch.nn.Module):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.descr = torch.nn.Sequential(
            torch.nn.Conv2d(1,8,kernel_size=4,stride=2), # (28 - 4)/2 + 1 -> 13
            torch.nn.BatchNorm2d(8),
            torch.nn.LeakyReLU(0.2),

            torch.nn.Conv2d(8,16,kernel_size=7,stride=2), # (13 - 7)/2 + 1 -> 4
            torch.nn.BatchNorm2d(16),
            torch.nn.LeakyReLU(0.2),

            torch.nn.Conv2d(16,1,kernel_size=4,stride=2), # (4-4)/2 + 1 = 1
            torch.nn.Sigmoid(),
        )

    def forward(self,x):
        return self.descr(x)

latent_dim=20
gen = Generator(latent_dim=latent_dim)
des = Descriminator()
lr = 1e-3
epochs = 5
genOpt = Adam(gen.parameters(),lr=lr,betas=(.5,.999))
desOpt = Adam(des.parameters(),lr=lr,betas=(.5,.999))

import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('agg')

for epoch in range(epochs):
    lossVal = 0
    Img =0
    for idx,(img,label) in tqdm(enumerate(train_loader),desc=f'Epoch[{epoch}/{epochs}]',total=len(train_loader)):

        genOpt.zero_grad()
        desOpt.zero_grad()

        # des gets 2 losses 1 -> real img loss , 2 -> fake img loss

        desOut = des(img).view(-1)
        labelDes = torch.ones_like(desOut).view(-1)
        real_loss = torch.nn.functional.binary_cross_entropy(desOut,labelDes,reduction='sum')
        
        batch_size = label.shape[0]
        generated = gen(torch.randn((batch_size,latent_dim,1,1)))
        label = (torch.ones(batch_size) * 0.1)
        fakeLabels = des(generated.detach()).view(-1) # it should categorise fakes as 0
        fakeloss = torch.nn.functional.binary_cross_entropy(fakeLabels,label,reduction='sum')
        des_loss = fakeloss + real_loss

        des_loss.backward()
        desOpt.step()
        # gen gets 1 loss -> generated by descriminator
        genLoss = torch.nn.functional.binary_cross_entropy(des(generated).view(-1),torch.ones_like(fakeLabels),reduction='sum')
        genLoss.backward()
        genOpt.step()
        Img = generated[0][0]
        lossVal += des_loss.item() + genLoss.item()

    print(f"Loss : {lossVal}")
    sample_img = Img.detach().cpu().numpy()  # No need to permute
    fig, axis = plt.subplots(1, 1, figsize=(4, 4))
    axis.imshow(sample_img, cmap='gray')  # Use grayscale colormap
    axis.axis('off')
    plt.savefig('current.png')
    plt.close(fig)
